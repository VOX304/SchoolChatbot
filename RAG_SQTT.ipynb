{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VOX304/SchoolChatbot/blob/main/RAG_SQTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X15KmGsrA-b9"
      },
      "source": [
        "#Packages setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T57Y2pVRXf6E",
        "outputId": "025e63fc-b20a-4414-e734-3f56860f82ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.44)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.17)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain \\\n",
        "langchain_community \\\n",
        "langchain_core \\\n",
        "langchain_google_genai \\\n",
        "python-dotenv \\\n",
        "pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLaao_cDlUls",
        "outputId": "c319a003-8133-4d29-90e4-e22fd4e82c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ZZGF2_Bc-z",
        "outputId": "5da2fce2-4885-480f-b1d8-a8e3564d8e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn \\\n",
        "numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTx2gGAeA508",
        "outputId": "ac3d643d-6764-4a18-af2d-c93dc994a30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cmFEYpAKhjHt"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "VietTransformer = SentenceTransformer('dangvantuan/vietnamese-document-embedding', trust_remote_code=True)\n",
        "#embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "pdf_files = ['/content/sample_data/KinhTevPhatTrien2024.pdf',\n",
        "             '/content/sample_data/KinhTevPhatTrien2025.pdf',\n",
        "             '/content/sample_data/KinhTevPhatTrien2025_t2.pdf']  # Adjust paths"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf_files)"
      ],
      "metadata": {
        "id": "SUh_b_f6VYIN",
        "outputId": "1fcb09d1-4646-497f-f7ab-7780b5cd2771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/sample_data/KinhTevPhatTrien2024.pdf', '/content/sample_data/KinhTevPhatTrien2025.pdf', '/content/sample_data/KinhTevPhatTrien2025_t2.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfRZiKG73qBP"
      },
      "source": [
        "#PDF-Preprocessing & VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9FNPA0Imh3TV",
        "outputId": "dd3acd06-11bf-4e5a-b15d-2c5af6261719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Extracted content saved to extracted_content.txt\n"
          ]
        }
      ],
      "source": [
        "documents = []\n",
        "for pdf in pdf_files:\n",
        "    pdf_loader = PyPDFLoader(pdf)\n",
        "    documents.extend(pdf_loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Ensure embeddings are generated correctly\n",
        "#embeddings = embedding_model.embed_documents([doc.page_content for doc in chunks])\n",
        "\n",
        "# Pass embedded vectors to FAISS\n",
        "\n",
        "with open(\"extracted_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        f.write(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\\n{'='*50}\\n\\n\")\n",
        "\n",
        "print(\"📄 Extracted content saved to extracted_content.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class CustomEmbedding:\n",
        "    def __init__(self, model_name):\n",
        "        self.model = SentenceTransformer(model_name, trust_remote_code=True)  # ✅ Add trust_remote_code=True\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.model.encode([text], convert_to_numpy=True)[0]\n",
        "    def __call__(self, text):\n",
        "        \"\"\"Make the class callable, so FAISS can use it.\"\"\"\n",
        "        return self.embed_query(text)\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = CustomEmbedding(\"dangvantuan/vietnamese-document-embedding\")\n",
        "\n",
        "# Generate embeddings\n",
        "doc_embeddings = embedding_model.embed_documents([\"Xin chào!\", \"Hà Nội là thủ đô của Việt Nam.\"])\n",
        "query_embedding = embedding_model.embed_query(\"Thành phố nào là thủ đô?\")\n",
        "\n",
        "print(doc_embeddings.shape, query_embedding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdIcGjKnAtk1",
        "outputId": "245c5ba8-fc79-4e1a-efe9-3d4373442c2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 768) (768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Create the FAISS vector store\n",
        "vector_db = FAISS.from_documents(chunks, embedding_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKX0S0mz_e4w",
        "outputId": "b2231a2a-49eb-44be-cd2d-1b002e65c799"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "z_Zy1uOzh0Yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f17ba40-bc8d-477e-da80-1683560c0800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed 3010 text chunks into FAISS vector database.\n"
          ]
        }
      ],
      "source": [
        "print(f\"✅ Processed {len(chunks)} text chunks into FAISS vector database.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "S6DA2G3NmVFl"
      },
      "outputs": [],
      "source": [
        "query = \"độ tuổi GenZ\"\n",
        "retrieved_docs = vector_db.similarity_search(query, k = 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7AL0_aPymcvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbc6a0a-48ed-49c8-ccd6-3be1171c3d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Document 1:\n",
            "Số 330 tháng 12/2024 66\n",
            "1. Giới thiệu\n",
            "Thế hệ Z thường được các nhà nghiên cứu xác định là sinh ra trong khoảng thời gian từ 1995 – 2012 \n",
            "(Barhate & Dirani, 2022; Maloni & cộng sự, 2019). Với việc được đào tạo trình độ đại học, nhóm này đã \n",
            "gia nhập thị trường lao động được khoảng 7 năm và đang dần trở thành lực lượng lao động chính, đặc biệt là \n",
            "trong lĩnh vực kinh doanh và quản lý. Trong bối cảnh chuyển đổi lực lượng lao động như vậy, rất cần thiết\n",
            "\n",
            "📄 Document 2:\n",
            "2.2. Thế hệ Z\n",
            "Là thế hệ mới nhất tham gia vào lực lượng lao động, thế hệ Z cho thấy sự khác biệt đáng kể trong hành \n",
            "vi và thái độ đối với công việc so với những thế hệ trước đây. Việc có thời gian đi học dài, có sự bao trùm \n",
            "của công nghệ và thiết bị di động, được sống trong một xã hội phát triển hơn đã tạo ra một thế hệ Z thiếu \n",
            "kinh nghiệm làm việc thực tế, coi trọng sự đa dạng và công bằng, dễ dàng rơi vào trạng thái lo âu và trầm\n",
            "\n",
            "📄 Document 3:\n",
            "sử dụng lại dịch vụ, giúp doanh nghệp có thể nâng cao doanh thu mà không tốn nhiều chi phí.\n",
            "Thế hệ Z là thế hệ sinh ra trong giai đoạn công nghệ phát triển, do đó thế hệ này có khả năng thích ứng \n",
            "nhanh chóng so với các thế hệ trước đó khi Việt Nam đẩy mạnh các hoạt động chuyển đổi số  và phát triển \n",
            "bền vững. Hơn thế nữa, đây là thế hệ có tiềm năng đóng góp rất lớn cho sự phát triển của nền kinh tế trong\n",
            "\n",
            "📄 Document 4:\n",
            "Số 330 tháng 12/2024 67\n",
            "Buchko, 2021), vì vậy mà các doanh nghiệp cần phải thấu hiểu để có thể đáp ứng những nhu cầu cũng như \n",
            "khai thác tối đa tiềm năng của thế hệ Z (Perilus, 2020).\n",
            "Trong thời gian gần đây, các nhà nghiên cứu bắt đầu tập trung hơn vào thế hệ Z ở các lĩnh vực như tiếp \n",
            "thị, du lịch và khởi sự kinh doanh (Doanh & Bernat, 2019; Nguyen & cộng sự, 2022; Nguyen & cộng sự, \n",
            "2021), nhưng vẫn chưa nhiều các nghiên cứu tập trung xem xét đối tượng này trong bối cảnh công việc hoặc\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(retrieved_docs[:4]):  # Show top 3\n",
        "    print(f\"\\n📄 Document {i+1}:\\n{doc.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xYlxyKNBV_p"
      },
      "source": [
        "#LLM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j4VY2rLjmpA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f53d161-a981-43dd-9201-9f82efaa520a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chat model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "    model=\"gemini-2.0-flash-thinking-exp-01-21\",\n",
        "    temperature=0.7\n",
        ")\n",
        "print(\"✅ Chat model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")"
      ],
      "metadata": {
        "id": "oUjvFMfNC1Ln"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n90n1D8qB9cR"
      },
      "source": [
        "#Augment_Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zXRBR2U7mziE"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "def augment_prompt(query):\n",
        "    # Get top 3 results from the knowledge base\n",
        "    results = vector_db.similarity_search(query, k=4)\n",
        "\n",
        "    # Extract text, sources, and pages\n",
        "    source_map = {}\n",
        "    for doc in results:\n",
        "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"Unknown\")\n",
        "        source_map[doc.page_content] = (source, page)\n",
        "\n",
        "    # Construct the augmented prompt\n",
        "    source_knowledge = \"\\n\".join(source_map.keys())\n",
        "\n",
        "    augmented_prompt = f\"\"\"Bạn là tư vấn viên của trường Sĩ Quan Thông Tin. Dựa vào nội dung tài liệu, hãy trả lời câu hỏi một cách chính xác và thân thiện bằng tiếng Việt.\n",
        "    Không thêm thông tin ngoài nội dung tài liệu. Nếu không tìm thấy câu trả lời trong tài liệu, chỉ cần nói rằng bạn không biết.\n",
        "\n",
        "    Nội dung tài liệu:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Câu hỏi:\n",
        "    {query}\"\"\"\n",
        "\n",
        "\n",
        "    return augmented_prompt, source_map\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdvgQND2CENP"
      },
      "source": [
        "#Answering Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxkV6QuupvcL"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# User question\n",
        "question = \"Độ tuổi Genz\"\n",
        "\n",
        "# Generate augmented prompt and retrieve sources\n",
        "context, source_map = augment_prompt(question)\n",
        "\n",
        "# Create human message for Gemini model\n",
        "prompt = HumanMessage(content=context)\n",
        "\n",
        "# Invoke Gemini model\n",
        "res = chat_model.invoke([prompt])\n",
        "response_text = res.content\n",
        "\n",
        "# Get embeddings for LLM response\n",
        "response_embedding = embedding_model.embed_query(response_text)\n",
        "\n",
        "# Track relevant sources\n",
        "relevant_sources = set()\n",
        "\n",
        "for text, (source, page) in source_map.items():\n",
        "    chunk_embedding = embedding_model.embed_query(text)  # Get embedding for each chunk\n",
        "    similarity_score = cosine_similarity([response_embedding], [chunk_embedding])[0][0]\n",
        "\n",
        "    if similarity_score >= 0.7:  # Threshold for relevance\n",
        "        relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "formatted_response = f\"Response: {response_text}\\nSources: {list(relevant_sources) if relevant_sources else ['No sources matched']}\"\n",
        "print(formatted_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_KAOaqY0Ez"
      },
      "source": [
        "#Generate Questions and Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WhJAu7udPkJ"
      },
      "outputs": [],
      "source": [
        "'''import time\n",
        "import csv\n",
        "import os\n",
        "\n",
        "csv_filename = \"generated_questions.csv\"\n",
        "\n",
        "# Open CSV file for writing with UTF-8 BOM\n",
        "with open(csv_filename, mode=\"a\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header (only \"Question\" column now)\n",
        "    writer.writerow([\"Question\"])\n",
        "\n",
        "    for i, doc in enumerate(pdf_files):\n",
        "        time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "        # Extract document name (without path)\n",
        "        doc_name = os.path.basename(doc)\n",
        "\n",
        "        # Select relevant chunks\n",
        "        doc_chunks = chunks[i * 3 : (i + 1) * 3]\n",
        "        doc_text = \"\\n\".join([chunk.page_content.strip() for chunk in doc_chunks]).strip()\n",
        "\n",
        "        if not doc_text:\n",
        "            continue  # Skip if the document has no text\n",
        "\n",
        "        # Generate questions with improved prompt\n",
        "        prompt = HumanMessage(content=f\"\"\"Dựa vào nội dung tài liệu \"{doc_name}\", hãy tạo 5 câu hỏi đa dạng bằng tiếng Việt.\n",
        "        Không đánh số thứ tự, không để lại khoảng trắng dư thừa, và mỗi câu hỏi phải có đủ ngữ cảnh để hiểu được tài liệu liên quan.\"\"\")\n",
        "\n",
        "        response = chat_model.invoke([prompt])\n",
        "        questions = [q.strip() for q in response.content.strip().split(\"\\n\") if q.strip()]  # Clean up questions\n",
        "\n",
        "        # Write each question to the CSV file\n",
        "        for question in questions:\n",
        "            writer.writerow([question])\n",
        "\n",
        "print(f\"✅ Questions saved to {csv_filename}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwAsxS4cZPOh"
      },
      "source": [
        "#Generate Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccqKpRxNZO-_"
      },
      "outputs": [],
      "source": [
        "'''import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "answer_csv = \"generated_QA.csv\"\n",
        "\n",
        "# Load generated questions\n",
        "df_questions = pd.read_csv(\"generated_questions.csv\")\n",
        "questions = df_questions[\"Question\"].tolist()\n",
        "\n",
        "answers = []\n",
        "answer_sources = []\n",
        "\n",
        "for question in questions:\n",
        "    time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "    # Retrieve relevant document content\n",
        "    context, _ = augment_prompt(question)\n",
        "    prompt = HumanMessage(content=context)\n",
        "\n",
        "    # Get answer from chatbot model\n",
        "    answer_res = chat_model.invoke([prompt])\n",
        "    answer = answer_res.content.strip()\n",
        "    answers.append(answer)\n",
        "\n",
        "    # Identify relevant sources for the answer\n",
        "    relevant_sources = set()\n",
        "    response_embedding = embedding_model.embed_query(answer)  # Embed answer\n",
        "\n",
        "    for text, (source, page) in source_map.items():\n",
        "        chunk_embedding = embedding_model.embed_query(text)  # Embed document chunk\n",
        "        similarity_score = cosine_similarity([response_embedding], [chunk_embedding])[0][0]\n",
        "\n",
        "        if similarity_score >= 0.7:  # Threshold for relevance\n",
        "            relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "    # Store relevant sources for answer\n",
        "    source_list = \"; \".join(relevant_sources) if relevant_sources else \"No sources matched\"\n",
        "    answer_sources.append(source_list)\n",
        "\n",
        "# Save answers and sources to CSV\n",
        "df_answers = pd.DataFrame({\"Answer\": answers, \"Relevant Source (Answer)\": answer_sources})\n",
        "df_answers.to_csv(answer_csv, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"✅ Answers saved to {answer_csv}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x3n1IQbCGND"
      },
      "source": [
        "#Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nU7jsd24CIUt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "# Load PhoRanker model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"itdainb/PhoRanker\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ak-3DIP-C2TV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d88055-e166-40bc-e601-005e09b7905e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8bssNE7OChd3"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"itdainb/PhoRanker\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sl-9GLpbCKN0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def rerank_documents(query, docs):\n",
        "    scored_docs = []\n",
        "    for doc in docs:\n",
        "        candidate_text = doc.page_content\n",
        "        # Prepare the input pair (query, candidate)\n",
        "        inputs = tokenizer(query, candidate_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Get the score (assuming a single output neuron for relevance)\n",
        "        score = outputs.logits[0][0].item()  # Access the first element directly\n",
        "        scored_docs.append((doc, score))\n",
        "\n",
        "    # Sort the documents by score (highest first)\n",
        "    scored_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
        "    # Return only the document objects in sorted order\n",
        "    return [doc for doc, score in scored_docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iP_8p0keCVNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322343f6-4402-4093-9dc1-4df08af82e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Document 1:\n",
            "Số 330 tháng 12/2024 66\n",
            "1. Giới thiệu\n",
            "Thế hệ Z thường được các nhà nghiên cứu xác định là sinh ra trong khoảng thời gian từ 1995 – 2012 \n",
            "(Barhate & Dirani, 2022; Maloni & cộng sự, 2019). Với việc được đào tạo trình độ đại học, nhóm này đã \n",
            "gia nhập thị trường lao động được khoảng 7 năm và đang dần trở thành lực lượng lao động chính, đặc biệt là \n",
            "trong lĩnh vực kinh doanh và quản lý. Trong bối cảnh chuyển đổi lực lượng lao động như vậy, rất cần thiết\n",
            "\n",
            "📄 Document 2:\n",
            "2.2. Thế hệ Z\n",
            "Là thế hệ mới nhất tham gia vào lực lượng lao động, thế hệ Z cho thấy sự khác biệt đáng kể trong hành \n",
            "vi và thái độ đối với công việc so với những thế hệ trước đây. Việc có thời gian đi học dài, có sự bao trùm \n",
            "của công nghệ và thiết bị di động, được sống trong một xã hội phát triển hơn đã tạo ra một thế hệ Z thiếu \n",
            "kinh nghiệm làm việc thực tế, coi trọng sự đa dạng và công bằng, dễ dàng rơi vào trạng thái lo âu và trầm\n",
            "\n",
            "📄 Document 3:\n",
            "sử dụng lại dịch vụ, giúp doanh nghệp có thể nâng cao doanh thu mà không tốn nhiều chi phí.\n",
            "Thế hệ Z là thế hệ sinh ra trong giai đoạn công nghệ phát triển, do đó thế hệ này có khả năng thích ứng \n",
            "nhanh chóng so với các thế hệ trước đó khi Việt Nam đẩy mạnh các hoạt động chuyển đổi số  và phát triển \n",
            "bền vững. Hơn thế nữa, đây là thế hệ có tiềm năng đóng góp rất lớn cho sự phát triển của nền kinh tế trong\n",
            "\n",
            "📄 Document 4:\n",
            "of Phoenix.\n",
            "Rudolph, C.W., & Zacher, H. (2017), ‘Considering generations from a lifespan developmental perspective’, Work, \n",
            "Aging and Retirement, 3(2), 113-129. DOI: https://doi.org/10.1093/workar/waw019. \n",
            "Schroth, H. (2019), ‘Are you ready for Gen Z in the workplace?’, California Management Review, 61(3), 5-18. DOI: \n",
            "https://doi.org/10.1177/0008125619841006. \n",
            "Twenge, J.M., Campbell, S.M., Hoffman, B.J., & Lance, C.E. (2010), ‘Generational differences in work values: Leisure\n",
            "\n",
            "📄 Document 5:\n",
            "Số 330 tháng 12/2024 67\n",
            "Buchko, 2021), vì vậy mà các doanh nghiệp cần phải thấu hiểu để có thể đáp ứng những nhu cầu cũng như \n",
            "khai thác tối đa tiềm năng của thế hệ Z (Perilus, 2020).\n",
            "Trong thời gian gần đây, các nhà nghiên cứu bắt đầu tập trung hơn vào thế hệ Z ở các lĩnh vực như tiếp \n",
            "thị, du lịch và khởi sự kinh doanh (Doanh & Bernat, 2019; Nguyen & cộng sự, 2022; Nguyen & cộng sự, \n",
            "2021), nhưng vẫn chưa nhiều các nghiên cứu tập trung xem xét đối tượng này trong bối cảnh công việc hoặc\n",
            "\n",
            "📄 Document 6:\n",
            "org/10.1016/S1574-0714(06)01003-7. \n",
            "Farrell, W.C., & Phungsoonthorn, T. (2020), ‘Generation Z in Thailand’, International Journal of Cross Cultural \n",
            "Management, 20(1), 25-51. DOI: https://doi.org/10.1177/1470595820904116. \n",
            "Gabrielova, K., & Buchko, A. A. (2021), ‘Here comes Generation Z: Millennials as managers’, Business Horizons, \n",
            "64(4), 489-499. DOI: https://doi.org/10.1016/j.bushor.2021.02.013.\n",
            "\n",
            "📄 Document 7:\n",
            "phải tìm hiểu về những mong muốn và kỳ vọng của sinh viên khối ngành kinh doanh và quản lý trong công \n",
            "việc – thế hệ Z tiếp theo sẽ bước vào thị trường lao động.\n",
            "Nhiều nghiên cứu đã chỉ ra rằng, thế hệ Z có những ưu tiên khác biệt so với các thế hệ trước đây về công \n",
            "việc, chẳng hạn như mong muốn có sự linh hoạt trong công việc, cơ hội phát triển cá nhân và tầm quan trọng \n",
            "của cân bằng giữa công việc và cuộc sống (Leslie & cộng sự, 2021; Osorio & Madero, 2024). Tuy nhiên,\n",
            "\n",
            "📄 Document 8:\n",
            "Tài liệu tham khảo\n",
            "Barhate, B., & Dirani, K.M. (2022), ‘Career aspirations of generation Z: a systematic literature review’, European \n",
            "Journal of Training and Development, 46(1/2), 139-157. DOI: 10.1108/EJTD-07-2020-0124.\n",
            "Becton, J.B., Walker, H.J., & Jones‐Farmer, A. (2014), ‘Generational differences in workplace behavior’, Journal of \n",
            "Applied Social Psychology, 44(3), 175-189. DOI: 10.1111/jasp.12208.\n",
            "\n",
            "📄 Document 9:\n",
            "generation’, Journal of Business and Psychology, 25, 281-292. DOI 10.1007/s10869-010-9159-4. \n",
            "Nguyen, C., Nguyen, T., & Luu, V . (2022), ‘Relationship between influencer marketing and purchase intention: \n",
            "focusing on Vietnamese gen Z consumers’, Independent Journal of Management & Production, 13(2), 810-828. \n",
            "DOI: https://doi.org/10.14807/ijmp.v13i2.1603. \n",
            "Nguyen, V . H., Truong, T. X. D., Pham, H. T., Tran, D. T., & Nguyen, P. H. (2021), ‘Travel intention to visit tourism\n",
            "\n",
            "📄 Document 10:\n",
            "Số 330 tháng 12/2024 70\n",
            "đánh giá cao. Điều này có thể được lý giải bởi thế hệ Z là một thế hệ thực tế (Gabrielova & Buchko, 2021), \n",
            "họ quan tâm đến những yếu tố hữu hình hơn là các yếu tố vô hình.\n",
            "Mặc dù không được xếp hàng đầu, nhưng nhìn chung các giá trị công việc đến từ bên trong như sự học \n",
            "hỏi, nâng cao kỹ năng, có thể nhìn thấy kết quả làm việc và khả năng sáng tạo trong công việc vẫn được coi\n",
            "\n",
            "📄 Document 11:\n",
            "destinations: A perspective of generation Z in Vietnam’, The Journal of Asian Finance, Economics and Business, \n",
            "8(2), 1043-1053. DOI: 10.13106/jafeb.2021.vol8.no2.1043. \n",
            "Osorio, M. L., & Madero, S. (2024), ‘Explaining Gen Z’s desire for hybrid work in corporate, family, and entrepreneurial \n",
            "settings’, Business Horizons, DOI: https://doi.org/10.1016/j.bushor.2024.02.008. \n",
            "Perilus, B. (2020), ‘Engaging four generations in the workplace: a single case study’, Doctoral dissertation, University \n",
            "of Phoenix.\n",
            "\n",
            "📄 Document 12:\n",
            "tương lai nói chung và ngành dịch vụ nói riêng. Xu hướng tiêu dùng của thế hệ Z đã và đang thay đổi, đòi \n",
            "hỏi sự đổi mới sáng tạo để bắt kịp và tạo thị trường mới. Vậy nên, nghiên cứu về đ ổi mới sáng tạo dịch vụ \n",
            "đối với khách hàng thế hệ Z có thể được ứng dụng không chỉ ở hiện tại mà còn ở tương lai.\n",
            "Vai trò của đổi mới sáng tạo dịch vụ đã được khám phá rộng rãi trong các nghiên cứu trước đây. Tuy \n",
            "nhiên, các nghiên cứu trước đây thường đánh giá tác động của đổi mới sáng tạo dịch vụ đối với sự hài lòng\n",
            "\n",
            "📄 Document 13:\n",
            "mới sáng tạo phù hợp với nhu cầu và đặc điểm của từng nhóm khách hàng thế hệ Z, thế hệ tiêu \n",
            "dùng tương lai tại Việt Nam.\n",
            "Từ khoá: Đổi mới dịch vụ, sự hài lòng của khách hàng, thế hệ Z, ý định mua lại.\n",
            "Mã JED: M1, M21\n",
            "The Influence of Service Innovation on The Satisfaction and Repurchases Intentions of \n",
            "Generation Z Customers\n",
            "Abstract\n",
            "This study aims to evaluate the impact of service innovation, including supportive service\n",
            "\n",
            "📄 Document 14:\n",
            "Việt Nam đặt làm ưu tiên trong quá trình tìm kiếm việc làm. Sự ưu tiên hàng đầu của sinh viên thế hệ Z là \n",
            "công việc đem lại mức lương cao và lộ trình thăng tiến rõ ràng. Điều này không chỉ phản ánh mong muốn \n",
            "về sự đảm bảo tài chính mà còn cho thấy sự quan tâm đến việc phát triển cá nhân trong sự nghiệp. Và không \n",
            "chỉ thế hệ Z mà các giá trị bên ngoài vẫn luôn được các thế hệ trước như là thế hệ X và thế hệ Y kỳ vọng\n",
            "\n",
            "📄 Document 15:\n",
            "rằng thế hệ Z coi trọng phúc lợi, kế hoạch hưu trí và khả năng dự đoán sự thay đổi, và hơn cả là phía doanh \n",
            "nghiệp cũng đồng tình về vấn đề này. Điều này không chỉ xuất hiện ở nền kinh tế mới nổi như Việt Nam mà \n",
            "đã xuất hiện trong bối cảnh nghiên cứu ở các quốc gia phát triển, giá trị công việc về sự ổn định ở đây được \n",
            "thế hệ Z rất coi trọng (Maloni & cộng sự, 2019). Cần lưu ý rằng, việc đề cao tính ổn định và vấn đề thường\n",
            "\n",
            "📄 Document 16:\n",
            "những kết nối của thế hệ Z hiện nay được thiết lập trên các nền tảng trực tuyến (Farrell & Phungsoonthorn, \n",
            "2020; Schroth, 2019), vì vậy mà họ ít quan tâm hơn đến những giá trị kết nối xã hội ở nơi làm việc. Ngoài \n",
            "ra, kết quả cũng có thể là gợi ý cho sự thay đổi giá trị xã hội và thời gian nghỉ ngơi của thế hệ Z và cần được \n",
            "khám phá thêm.\n",
            "Khác với nhận định thông thường rằng thế hệ Z ít quan tâm tới sự ổn định, kết quả nghiên cứu chỉ ra\n",
            "\n",
            "📄 Document 17:\n",
            "cảm, thiếu kỹ năng giao tiếp (Schroth, 2019). Không chỉ khác biệt với các thế hệ khác, ngay trong thế hệ Z \n",
            "cũng có sự khác biệt đáng kể với những nhận thức và giá trị khác nhau như: cân bằng cuộc sống-công việc, \n",
            "môi trường làm việc thoải mái và sự thăng tiến trong nghề nghiệp (Leslie & cộng sự, 2021). Điều này có thể \n",
            "dẫn đến những xung đột tiềm tàng trong doanh nghiệp giữa quản lý và nhân viên thế hệ Z (Gabrielova &\n",
            "\n",
            "📄 Document 18:\n",
            "cũng tập trung chỉ ra các chiến lược phát triển con người riêng cho từng thế hệ và cách kết hợp các thế hệ \n",
            "với nhau để tạo ra giá trị cho doanh nghiệp (Callanan & Greenhaus, 2008; Chaudhuri & Ghosh, 2012; Ng \n",
            "& cộng sự, 2010).\n",
            "Tuy nhiên, các nghiên cứu sử dụng Lý thuyết Thế hệ thường tập trung vào sự khác biệt và kết nối giữa các \n",
            "thế hệ bên trong doanh nghiệp mà còn ít xem xét đến trường hợp sự khác biệt của những người bên trong \n",
            "doanh nghiệp và những người sắp bước chân vào doanh nghiệp.\n",
            "2.2. Thế hệ Z\n",
            "\n",
            "📄 Document 19:\n",
            "Hampton, D., & Welsh, D. (2019), ‘Work values of Generation Z nurses’, JONA: The Journal of Nursing Administration, \n",
            "49(10), 480-486. DOI: 10.1097/NNA.0000000000000791. \n",
            "Hansen, J.-I. C., & Leuty, M.E. (2012), ‘Work values across generations’, Journal of Career Assessment, 20(1), 34-52. \n",
            "DOI: https://doi.org/10.1177/1069072711417163. \n",
            "Hurst, J.L., & Good, L.K. (2009), ‘Generation Y and career choice: The impact of retail career perceptions,\n",
            "\n",
            "📄 Document 20:\n",
            "ngược trong nhìn nhận các khía cạnh của quản lý trực tiếp còn có thể lý giải bởi những khác biệt trong quan \n",
            "điểm giá trị công việc của thế hệ Y (đang là quản lý) và thế hệ Z (đang là nhân viên) (Gabrielova & Buchko, \n",
            "2021).\n",
            "Thay vì sự khác biệt được tạo thành trên cơ sở sinh viên đánh giá giá trị cao hơn doanh nghiệp, thì kết \n",
            "quả lại cho thấy điều ngược lại. Ở các giá trị về xã hội và thời gian nghỉ ngơi, dường như doanh nghiệp đang\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "📄 Document 1:\n",
            "Số 330 tháng 12/2024 66\n",
            "1. Giới thiệu\n",
            "Thế hệ Z thường được các nhà nghiên cứu xác định là sinh ra trong khoảng thời gian từ 1995 – 2012 \n",
            "(Barhate & Dirani, 2022; Maloni & cộng sự, 2019). Với việc được đào tạo trình độ đại học, nhóm này đã \n",
            "gia nhập thị trường lao động được khoảng 7 năm và đang dần trở thành lực lượng lao động chính, đặc biệt là \n",
            "trong lĩnh vực kinh doanh và quản lý. Trong bối cảnh chuyển đổi lực lượng lao động như vậy, rất cần thiết\n",
            "\n",
            "📄 Document 2:\n",
            "sử dụng lại dịch vụ, giúp doanh nghệp có thể nâng cao doanh thu mà không tốn nhiều chi phí.\n",
            "Thế hệ Z là thế hệ sinh ra trong giai đoạn công nghệ phát triển, do đó thế hệ này có khả năng thích ứng \n",
            "nhanh chóng so với các thế hệ trước đó khi Việt Nam đẩy mạnh các hoạt động chuyển đổi số  và phát triển \n",
            "bền vững. Hơn thế nữa, đây là thế hệ có tiềm năng đóng góp rất lớn cho sự phát triển của nền kinh tế trong\n",
            "\n",
            "📄 Document 3:\n",
            "Số 330 tháng 12/2024 67\n",
            "Buchko, 2021), vì vậy mà các doanh nghiệp cần phải thấu hiểu để có thể đáp ứng những nhu cầu cũng như \n",
            "khai thác tối đa tiềm năng của thế hệ Z (Perilus, 2020).\n",
            "Trong thời gian gần đây, các nhà nghiên cứu bắt đầu tập trung hơn vào thế hệ Z ở các lĩnh vực như tiếp \n",
            "thị, du lịch và khởi sự kinh doanh (Doanh & Bernat, 2019; Nguyen & cộng sự, 2022; Nguyen & cộng sự, \n",
            "2021), nhưng vẫn chưa nhiều các nghiên cứu tập trung xem xét đối tượng này trong bối cảnh công việc hoặc\n",
            "\n",
            "📄 Document 4:\n",
            "org/10.1016/S1574-0714(06)01003-7. \n",
            "Farrell, W.C., & Phungsoonthorn, T. (2020), ‘Generation Z in Thailand’, International Journal of Cross Cultural \n",
            "Management, 20(1), 25-51. DOI: https://doi.org/10.1177/1470595820904116. \n",
            "Gabrielova, K., & Buchko, A. A. (2021), ‘Here comes Generation Z: Millennials as managers’, Business Horizons, \n",
            "64(4), 489-499. DOI: https://doi.org/10.1016/j.bushor.2021.02.013.\n"
          ]
        }
      ],
      "source": [
        "query = \"GenZ sinh ra trong khoảng thời gian nào?\"\n",
        "# Retrieve candidates (increase k to get more candidates for reranking)\n",
        "retrieved_docs = vector_db.similarity_search(query, k=20)\n",
        "\n",
        "# Rerank the documents using PhoRanker\n",
        "reranked_docs = rerank_documents(query, retrieved_docs)\n",
        "\n",
        "# Now you can use the top reranked documents (e.g., top 3)\n",
        "top_docs = reranked_docs[:4]\n",
        "\n",
        "# Print out the top documents for inspection\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\n📄 Document {i+1}:\\n{doc.page_content}\")\n",
        "\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "for i, doc in enumerate(top_docs):\n",
        "    print(f\"\\n📄 Document {i+1}:\\n{doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_prompt_with_reranker(query):\n",
        "    # Retrieve a larger set of candidate docs\n",
        "    candidates = vector_db.similarity_search(query, k=15)\n",
        "    # Rerank them using PhoRanker\n",
        "    reranked_candidates = rerank_documents(query, candidates)\n",
        "\n",
        "    # Chọn top 4 tài liệu sau khi rerank\n",
        "    selected_docs = reranked_candidates[:4]\n",
        "\n",
        "    # Tạo source map và trích dẫn nguồn\n",
        "    source_map = {}\n",
        "    formatted_sources = []\n",
        "\n",
        "    for idx, doc in enumerate(selected_docs, start=1):\n",
        "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"Unknown\")\n",
        "        source_map[doc.page_content] = (source, page)\n",
        "        formatted_sources.append(f\"[{idx}] {source} - Trang {page}\")\n",
        "\n",
        "    # Nội dung tài liệu dùng để trả lời\n",
        "    source_knowledge = \"\\n\\n\".join(f\"({i+1}) {doc.page_content}\" for i, doc in enumerate(selected_docs))\n",
        "    citation_info = \"\\n\".join(formatted_sources)\n",
        "\n",
        "    # Prompt tối ưu\n",
        "    augmented_prompt = f\"\"\"Bạn là tư vấn viên của trường VGU. Hãy trả lời câu hỏi một cách chính xác, thân thiện, và trích dẫn nguồn gốc thông tin.\n",
        "Bạn chỉ được sử dụng thông tin từ tài liệu dưới đây, không thêm nội dung không có trong tài liệu. Nếu không tìm thấy câu trả lời, chỉ cần nói rằng bạn không biết.\n",
        "\n",
        "📌 **Nội dung tài liệu trích xuất**:\n",
        "{source_knowledge}\n",
        "\n",
        "❓ **Câu hỏi**:\n",
        "{query}\n",
        "\n",
        "📖 **Nguồn tài liệu**:\n",
        "{citation_info}\n",
        "\"\"\"\n",
        "\n",
        "    print(candidates)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    return augmented_prompt, source_map\n"
      ],
      "metadata": {
        "id": "yGEx-BMycsva"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder  # ✅ NEW\n",
        "import numpy as np\n",
        "\n",
        "# Load Cross-Encoder model\n",
        "cross_encoder = CrossEncoder(\"itdainb/PhoRanker\")"
      ],
      "metadata": {
        "id": "7hYuR-BVLxwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # ✅ Use a ranking model\n",
        "\n",
        "# User question\n",
        "question = \"độ tuổi GenZ\"\n",
        "\n",
        "# Retrieve chunks & sources\n",
        "context, source_map = augment_prompt_with_reranker(question)\n",
        "\n",
        "# Invoke LLM\n",
        "prompt = HumanMessage(content=context)\n",
        "res = chat_model.invoke([prompt])\n",
        "response_text = res.content  # LLM-generated response\n",
        "'''\n",
        "# Track relevant sources\n",
        "relevant_sources = set()\n",
        "\n",
        "# Score each retrieved chunk using Cross-Encoder\n",
        "scores = {}\n",
        "for text, (source, page) in source_map.items():\n",
        "    score = cross_encoder.predict([(question, text)])[0]  # ✅ Cross-Encoder scoring\n",
        "    scores[(source, page)] = score\n",
        "\n",
        "# Sort & filter sources by relevance threshold\n",
        "sorted_sources = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "for (source, page), score in sorted_sources:\n",
        "    if score >= 0.5:  # ✅ Adjust threshold as needed\n",
        "        relevant_sources.add(f\"{source} (Page {page+1})\")\n",
        "\n",
        "# Format response\n",
        "formatted_response = f\"Response: {response_text}\\nSources: {list(relevant_sources) if relevant_sources else ['No sources matched']}\"\n",
        "print(formatted_response)'''\n",
        "print(response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Jl6zEFfgc__t",
        "outputId": "47290475-b63e-429f-c332-81a6d6cf2a55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vector_db' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3c8a9b366767>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Retrieve chunks & sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_prompt_with_reranker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Invoke LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4c8738ad666d>\u001b[0m in \u001b[0;36maugment_prompt_with_reranker\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maugment_prompt_with_reranker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Retrieve a larger set of candidate docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Rerank them using PhoRanker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreranked_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrerank_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vector_db' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VrLvxLhtLe12"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMYO5NCpsC6d3liqmTatl8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}